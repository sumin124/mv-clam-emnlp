# MV-CLAM
MV-CLAM is a framework designed to generate molecular captions by effectively aligning 2D and 3D molecular representations with text. At its core, MQ-Former serves as a cross-modal projector, combining multi-view molecular embeddings into a unified token for seamless language model interpretationâ€‹.

<div style="display: flex; justify-content: space-between;">
  <img width="35%" src="https://github.com/user-attachments/assets/6b256641-0db5-4977-b6c5-79fc170ac5c2" alt="Figure 2">
  <img width="60%" src="https://github.com/user-attachments/assets/0182f23f-d608-4b4d-aabe-66283ea1660f" alt="Figure 3">
</div>


The left figure illustrates the overall framework of MV-CLAM, highlighting how multi-view molecular representations (2D and 3D structures) are processed through specialized encoders and aligned with textual data via the MQ-Former before generating molecular captions using a language model. 
The right figure provides a detailed view of the internal structure of the MQ-Former, demonstrating its use of shared self-attention layers and cross-modal projection to consolidate 2D and 3D molecular embeddings into a unified representation optimized for language model interpretation.

# Environment 
Install `conda` environment

```
conda env create -f mvclam.yaml
```

# Usage
- `mol_stage1.py`: Trains the MQ-Former to align multi-view molecular representations (2D and 3D structures) with textual descriptions.
  It uses multi-objective learning with molecule-text contrastive loss, matching loss, and captioning loss to generate high-quality universal query tokens.
  
  **Stage 1 Pretrain**
  ```
  python mol_stage1.py --num_query_token 12 --warmup_steps 200 --filename stage1_pretrain --seed 42--max_epochs 35 --mode pretrain
  ```
  Pretrains MQ-Former on a PubChem pretrain dataset to initialize its ability to align 2D and 3D molecular representations with textual data.

  **Stage 1 Train**
  ```
  python mol_stage1.py --num_query_token 12 --warmup_steps 200 --filename stage1_train --seed 42--max_epochs 10 --mode ft --stage1_path stage1_pretrain_output_ckpt
  ```
  Furthre train MQ-Former on a PubChem train dataset.

- `mol_stage2.py`:Fine-tunes the LLaMA2 language model for molecule captioning tasks. Using the universal query tokens generated by MQ-Former, it optimizes the language model to generate detailed molecular descriptions based on aligned 2D and 3D embeddings.

  **Stage 2 Pretrain**
  
  ```
  python mol_stage2.py --num_query_token 12 --warmup_steps 200 --filename stage2_pretrain --seed 42--max_epochs 10 --mode pretrain --lora_r 8 --stage1_path stage1_train_output_ckpt
  ```
  Fine-tunes the pretrained MQ-Former and LLaMA2 model using the PubChem pretrain dataset for molecule captioning.

  **Stage 2 Train**
  ```
  python mol_stage2.py --num_query_token 12 --warmup_steps 200 --filename stage2_train --seed 42--max_epochs 10 --mode ft --stage2_path stage2_pretrain_output_ckpt
  ```
  Further train the MQ-Former and LLaMA2 model using the PubChem train dataset.
